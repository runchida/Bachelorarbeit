\chapter[Introduction]{Introduction}

In recent years, machine learning has obtained an increasingly important role in many fields among researchers, industrial leaders and service providers. Although shallow machine learning methods work impressively with structured data and can solve many problems already, the rapid growth of computational power and complexity of the problems emerging from various use cases call back the popularity of deep learning method such as neural network. Being a part of machine learning, deep learning methods also solve problems with little human intervention using given data. The main difference is that these methods involve nested layers of neural networks which can tackle more complicated tasks or higher-dimensional data than other machine learning methods so far and promise better results if proper conditions are met. 

While neural networks can perform better than standard machine learning algorithms and require less structure in the data, they also require more resources such as computational power and amount of data for training. As said above, recent technological developments already promise continuous improvement in performances of computers in the foreseeable future. Another challenge faced in the development is to obtain the data necessary to unfold the potential of the networks. Generally, gathering more data increases costs and time in a project and in some cases even impossible. Therefore, it is more effective to find a way to use existing data to train a system for another scenario where we have less access to the data or to generalize a trained network so that it can also solve another task related to the original one. These are the motivations of domain adaptation.

In case of the domain adaptation in an image classification task, this involves training a system on one or multiple sets of images to classify objects in another sets of image. Being collected and captured under different conditions, these images sets are of different data distribution and therefore represent different domains of the data. The source domain is the dataset on which the system should train while the dataset the system should classify is referred to as the target domain. The domain adaptation methods should should be able to adapt the system to these differences between the source it has seen and the unknown target. Most of the previous works are done based on the setup where there is a complete representation of all the objects to be classified in the source domain. This is however not always the case in reality because obtaining images of all presenting objects under only one domain can be ineffective. For example, when different cameras used to take these picture provide image sets of different domains, this would mean at least one camera will have to collect a complete collection of all the objects we want to classify. Therefore, this overlooked scenario of incomplete sets of data has to be looked upon like it is done in \cite{domainMixture}. 

In this thesis, we investigate the Domain Mixture scenario using another neural architecture called Domain Separation Network \cite{DSN}, in comparison to \cite{domainMixture} in which the authors use the Domain Adversarial Neural Network \cite{dannGanin}. Our goal is to evaluate the performance of this architecture in this overlooked scenario and to see how domain mixture effects it. By mixing different datasets into one source domain, different data distributions must be accounted for unlike a source domain with a single dataset. This difference to the mostly assumed case of domain adaptation might cause different performance of the same domain adaptation methods. In our experiments, we use the datasets of handwritten digit images to create this scenario. The visualization of this scenario on the chosen dataset are in Figure \ref{fig:compdomainDA}. The DSN, similar to the DANN, also tries to create domain-free representations on which the classifier can train on by extracting at the same time the mutual representations of both domains. In addition, the DSN also extracts domain-specific representations for each domains to further purify the domain-free representations from noises originated from the domain-relevant features. Therefore, it can be expected that the DSN could cope well with this scenario since it also should be able to distinguish these aspects of the inputs even though the presented objects in the source domain are of different characters.

\begin{figure}[tbh]
  \centering
    \includegraphics[width=\textwidth]{abbildungen/compdomainMixture.png}
    \caption{Complete domain mixture scenario on MNIST and MNIST-M: In this scenario, there are completely supervised representations of all 10 classes created by combining samples from both domains. It is also possible that the supervised samples of some classes are present in both domains. For all the remaining class-domain combinations, there are unsupervised samples. The supervised class-domain combinations are marked with O and the unsupervised ones with X.} 
  \label{fig:compdomainDA}
\end{figure}

In order to understand the experiments, it is necessary to first understand the basics of the neural networks. In the first section, we will first introduce the basics of the neural networks: the elementary components, the relevant structures to our problem and the network optimization methods. Since the scenario investigated is inspired from a real-world problem, we also introduce real-world applications of the relevant neural structure, the Convolutional Neural Network (CNN) and the Autoencoders. Then, in Section \ref{ch:domainAdaptation}, the domain adaptation problems will be explained along with theirs causes, the dataset shifts. Also, the highlight scenario, the Domain Mixture Scenario will be explained and discussed in detail. After that, Section \ref{ch:dsn} will summarize the Domain Separation Networks with all its structural components such as the autoencoder and the loss functions used. With the knowledge about the problem, the experimental setups and the results will be explained and discussed in Section \ref{ch:experiment} followed by the conclusion of the work in Section \ref{ch:conclusion}.


