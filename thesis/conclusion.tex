\chapter{Conclusion} \label{ch:conclusion}
With increasing popularity of deep neural networks, data collection has become more crucial at the same time. Fortunately, the internet and the new technologies allow us to collectively share our data over the world and over time. For computer vision, new camera models and those coming with mobile phones allow even more possibilities to capture images all the time and all over the places, which many of us also do. With all the data coming in, explicitly collecting data can be limited if, and only if, we can make use of this data. As already studied, using these images of different natures without taking their diversities into account are not effective and could make things worse. This possibility of using existing data or trained networks motivates the development of domain adaptation methods to optimize this opportunity. 

Many theories and networks are already created to apply this method to real problems. Mostly, however, the scenarios investigated involve adapting one domain to another, which represents the reality only limitedly. The images in the real world come from many different conditions and capturing methods. Even doing researches, the datasets previously collected each have their own biases. Only assuming that there will be only one domain in the training dataset is not realistic and even lead to sub-optimal performance. For example, some methods applying domain adaptation do not account for the different distributions of the domains and if existing domains are all treated as one training domain, these differences do not fully support the adaptation as it could be. Though still a minority, there are also studies on domain adaptation with multiple sources which describe the problem more realistically. They propose methods accounting for the existing diversity in the source domain to allow for better knowledge extraction which results in better domain adaptation at the end. 

Another still overlooked scenario in domain adaptation is also when there are many domains acting as a source domain and they are not complete, i.e., they do not have representations of all the classes that should be classified. This situation is even nearer to the reality since we cannot expect all image sources to have the same kinds of image and all the types of object we want. By trying to classify more and more objects, it is plausible to assume that more and more data source will have to be merged to cover all the target objects. 

In this thesis, we focused our work on a specific case of this scenario, the complete domain mixture scenario. The theories on the neural networks, domain adaptation and the involving problems are introduced for better understanding of the experiments. Also, in addition to the theoretical explanation, examples of the real usages or previous studies were integrated to bring the practicality into the theories. Then, the experiments on this scenario were carried with the domain separation network, a network proposed to tackle the domain adaptation by leaning to extract the domain information in order to learn the object information independently. Based on the results presented in \cite{DSN} and \cite{domainMixture}, interesting insights that need further inspections were found. The performance of the DSN' without the similarity loss is noticeably lower than when the loss is activated which further confirms the effectiveness of the domain-free representations in applying domain adaptation. On the other hand, the reconstruction loss and the difference loss seem to be complementary to the similarity in making better representations without contamination from the domain aspects of the inputs. Theses losses, when activated solely, offer averagely the same accuracy as if no domain adaptation is applied in the complete domain mixture scenario. Furthermore, the DSN' presents a performance gap in this scenario between supervised classes $m = 5$ and $m = 6$. Since overfitting seems, at least at first glance, not to be the cause, further investigation is still needed. The experiments should be repeated again with training batches containing equal amount of supervised and unsupervised data to ensure a constant training of the classifier. Also, the unstable results between runs with the same setups when less supervised images are involved i.e. $m \leq 6$, can be observed. Partly, the accuracy is close to 0. Possible reasons of this should also be validated. The confusion matrix, for instance, can be calculated to gain more information about the wrong predictions.

The complete domain mixture scenario is only a special case of the domain mixture scenario. Conditioned to offer the data for all the competing classes, the training is simplified since the system has 'seen' all types of objects under every involving domains even if the information is not supervised. Another special condition are only partly supervised domains. In the reality, it is possible that some of the domains are either fully supervised or lack completely of data on particular objects. Generally, the domain mixture scenario is highly related to the real-world situations. The completeness and incompleteness of the datasets cannot always be predicted or ordered as will. So far to the best knowledge, there are still significantly less works done similarly on the scenarios with mixed domains than on those with complete domains. Therefore, it is encouraged to study this scenario more to better understand of the reality and its potential use. The opportunity to use the data under this particular scenario is huge and will be worth the work. 